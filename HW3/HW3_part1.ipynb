{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1LqrT0xWEoA"
      },
      "outputs": [],
      "source": [
        "# COMP 548 - MEDICAL IMAGE ANALYSIS HOMEWORK #3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6P2vTdIWSTi",
        "outputId": "465bed3c-a17e-46db-9143-f70706af7454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RN5CzU1DEEl",
        "outputId": "27c49170-5539-40fe-b9d1-cb4a8d5b6d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/868.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.0/868.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.3.0+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torchmetrics-1.4.0.post0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvnLRjWAWUbk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize, ToPILImage\n",
        "from torchvision.io import read_image\n",
        "from torchvision import transforms\n",
        "from torchmetrics.classification import BinaryPrecision, BinaryRecall, BinaryF1Score\n",
        "import statistics\n",
        "\n",
        "import torch.utils.data\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, Subset\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import copy\n",
        "import multiprocessing as mp\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOTUBRRkWXaY"
      },
      "outputs": [],
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "mp.set_start_method('spawn', force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MftvcJzC6GTf",
        "outputId": "70e1cbc4-8ec7-4db2-f248-0b868c14d803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "253\n",
            "0\n",
            "L\n",
            ".jpg\n"
          ]
        }
      ],
      "source": [
        "im_path = '.../path/...'\n",
        "im = Image.open(im_path)\n",
        "image_array = np.array(im)\n",
        "print(image_array.max())\n",
        "print(image_array.min())\n",
        "print(im.mode)\n",
        "_, file_extension = os.path.splitext(im_path)\n",
        "print(file_extension)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Srg5fW1o6HGv",
        "outputId": "93cb3d9b-b2dc-4a61-e0ff-a5be9ba3cb65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "0\n",
            "L\n",
            ".png\n"
          ]
        }
      ],
      "source": [
        "im_gold_path = '.../path/...'\n",
        "im_gold = Image.open(im_gold_path)\n",
        "image_gold_array = np.array(im_gold)\n",
        "print(image_gold_array.max())\n",
        "print(image_gold_array.min())\n",
        "print(im_gold.mode)\n",
        "_, file_gold_extension = os.path.splitext(im_gold_path)\n",
        "print(file_gold_extension)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4EzFPU7Yh9s",
        "outputId": "34de8044-e1b0-4702-fb47-72ec2a7647cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gold standard unique values: [0 1]\n"
          ]
        }
      ],
      "source": [
        "unique_values = np.unique(image_gold_array)\n",
        "print(f\"Gold standard unique values: {unique_values}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbkFteXO0QiT"
      },
      "outputs": [],
      "source": [
        "training_dataset_dir = '.../path/...'\n",
        "validation_dataset_dir = '.../path/...'\n",
        "test_dataset_dir = '.../path/...'\n",
        "\n",
        "gold_training_dir = '.../path/...'\n",
        "gold_validation_dir = '.../path/...'\n",
        "gold_test_dir = '.../path/...'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, image_dir, gold_dir):\n",
        "        self.image_dir = image_dir\n",
        "        self.gold_dir = gold_dir\n",
        "        self.images = sorted(os.listdir(image_dir))\n",
        "        self.gold_images = sorted(os.listdir(gold_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.image_dir, self.images[idx])\n",
        "        gold_name = os.path.join(self.gold_dir, self.gold_images[idx])\n",
        "\n",
        "        image = Image.open(img_name)\n",
        "        gold_standard = Image.open(gold_name)\n",
        "\n",
        "        image_np = np.array(image).astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
        "        gold_standard_np = np.array(gold_standard).astype(np.float32)  # Do not normalize gold standards!!!!!!\n",
        "\n",
        "        image_tensor = torch.from_numpy(image_np).unsqueeze(0)\n",
        "        gold_standard_tensor = torch.from_numpy(gold_standard_np).unsqueeze(0)\n",
        "\n",
        "        return image_tensor, gold_standard_tensor, self.images[idx], self.gold_images[idx]"
      ],
      "metadata": {
        "id": "rHdXtEW5z3Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOCoi98a8gCS"
      },
      "outputs": [],
      "source": [
        "train_dataset = MyDataset(training_dataset_dir, gold_training_dir)\n",
        "val_dataset = MyDataset(validation_dataset_dir, gold_validation_dir)\n",
        "test_dataset = MyDataset(test_dataset_dir, gold_test_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmwWI13f9ppR",
        "outputId": "a72d1750-031b-40fe-cec6-2fe8add53dbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CT Image Filenames:\n",
            "['CT_11_Image_47.jpg', 'CT_11_Image_48.jpg', 'CT_11_Image_49.jpg', 'CT_11_Image_50.jpg', 'CT_11_Image_51.jpg', 'CT_11_Image_52.jpg', 'CT_11_Image_53.jpg', 'CT_11_Image_54.jpg', 'CT_11_Image_55.jpg', 'CT_11_Image_56.jpg', 'CT_11_Image_57.jpg', 'CT_11_Image_58.jpg', 'CT_11_Image_59.jpg', 'CT_11_Image_60.jpg', 'CT_11_Image_61.jpg', 'CT_11_Image_62.jpg', 'CT_12_Image_49.jpg', 'CT_12_Image_50.jpg', 'CT_12_Image_51.jpg', 'CT_12_Image_52.jpg', 'CT_12_Image_53.jpg', 'CT_12_Image_54.jpg', 'CT_12_Image_55.jpg', 'CT_12_Image_56.jpg', 'CT_12_Image_57.jpg', 'CT_12_Image_58.jpg', 'CT_12_Image_59.jpg', 'CT_12_Image_60.jpg', 'CT_12_Image_61.jpg', 'CT_13_Image_42.jpg', 'CT_13_Image_43.jpg', 'CT_13_Image_44.jpg', 'CT_13_Image_45.jpg', 'CT_13_Image_46.jpg', 'CT_13_Image_47.jpg', 'CT_13_Image_48.jpg', 'CT_13_Image_49.jpg', 'CT_13_Image_50.jpg', 'CT_13_Image_51.jpg', 'CT_13_Image_52.jpg', 'CT_13_Image_53.jpg', 'CT_13_Image_54.jpg', 'CT_13_Image_55.jpg', 'CT_13_Image_56.jpg', 'CT_13_Image_57.jpg', 'CT_13_Image_58.jpg', 'CT_14_Image_44.jpg', 'CT_14_Image_45.jpg', 'CT_14_Image_46.jpg', 'CT_14_Image_47.jpg', 'CT_14_Image_48.jpg', 'CT_14_Image_49.jpg', 'CT_14_Image_50.jpg', 'CT_14_Image_51.jpg', 'CT_14_Image_52.jpg', 'CT_14_Image_53.jpg', 'CT_14_Image_54.jpg', 'CT_14_Image_55.jpg', 'CT_14_Image_56.jpg', 'CT_14_Image_57.jpg', 'CT_15_Image_54.jpg', 'CT_15_Image_55.jpg', 'CT_15_Image_56.jpg', 'CT_15_Image_57.jpg', 'CT_15_Image_58.jpg', 'CT_15_Image_59.jpg', 'CT_15_Image_60.jpg', 'CT_15_Image_61.jpg', 'CT_15_Image_62.jpg', 'CT_15_Image_63.jpg', 'CT_15_Image_64.jpg', 'CT_15_Image_65.jpg', 'CT_15_Image_66.jpg', 'CT_15_Image_67.jpg', 'CT_15_Image_68.jpg', 'CT_15_Image_69.jpg', 'CT_15_Image_70.jpg', 'CT_15_Image_71.jpg', 'CT_15_Image_72.jpg', 'CT_15_Image_73.jpg', 'CT_15_Image_74.jpg', 'CT_15_Image_75.jpg']\n",
            "Gold Standard Image Filenames:\n",
            "['gold_11_Image_47.png', 'gold_11_Image_48.png', 'gold_11_Image_49.png', 'gold_11_Image_50.png', 'gold_11_Image_51.png', 'gold_11_Image_52.png', 'gold_11_Image_53.png', 'gold_11_Image_54.png', 'gold_11_Image_55.png', 'gold_11_Image_56.png', 'gold_11_Image_57.png', 'gold_11_Image_58.png', 'gold_11_Image_59.png', 'gold_11_Image_60.png', 'gold_11_Image_61.png', 'gold_11_Image_62.png', 'gold_12_Image_49.png', 'gold_12_Image_50.png', 'gold_12_Image_51.png', 'gold_12_Image_52.png', 'gold_12_Image_53.png', 'gold_12_Image_54.png', 'gold_12_Image_55.png', 'gold_12_Image_56.png', 'gold_12_Image_57.png', 'gold_12_Image_58.png', 'gold_12_Image_59.png', 'gold_12_Image_60.png', 'gold_12_Image_61.png', 'gold_13_Image_42.png', 'gold_13_Image_43.png', 'gold_13_Image_44.png', 'gold_13_Image_45.png', 'gold_13_Image_46.png', 'gold_13_Image_47.png', 'gold_13_Image_48.png', 'gold_13_Image_49.png', 'gold_13_Image_50.png', 'gold_13_Image_51.png', 'gold_13_Image_52.png', 'gold_13_Image_53.png', 'gold_13_Image_54.png', 'gold_13_Image_55.png', 'gold_13_Image_56.png', 'gold_13_Image_57.png', 'gold_13_Image_58.png', 'gold_14_Image_44.png', 'gold_14_Image_45.png', 'gold_14_Image_46.png', 'gold_14_Image_47.png', 'gold_14_Image_48.png', 'gold_14_Image_49.png', 'gold_14_Image_50.png', 'gold_14_Image_51.png', 'gold_14_Image_52.png', 'gold_14_Image_53.png', 'gold_14_Image_54.png', 'gold_14_Image_55.png', 'gold_14_Image_56.png', 'gold_14_Image_57.png', 'gold_15_Image_54.png', 'gold_15_Image_55.png', 'gold_15_Image_56.png', 'gold_15_Image_57.png', 'gold_15_Image_58.png', 'gold_15_Image_59.png', 'gold_15_Image_60.png', 'gold_15_Image_61.png', 'gold_15_Image_62.png', 'gold_15_Image_63.png', 'gold_15_Image_64.png', 'gold_15_Image_65.png', 'gold_15_Image_66.png', 'gold_15_Image_67.png', 'gold_15_Image_68.png', 'gold_15_Image_69.png', 'gold_15_Image_70.png', 'gold_15_Image_71.png', 'gold_15_Image_72.png', 'gold_15_Image_73.png', 'gold_15_Image_74.png', 'gold_15_Image_75.png']\n"
          ]
        }
      ],
      "source": [
        "ct_image_filenames = test_dataset.images\n",
        "print(\"CT Image Filenames:\")\n",
        "print(ct_image_filenames)\n",
        "\n",
        "gold_image_filenames = test_dataset.gold_images\n",
        "print(\"Gold Standard Image Filenames:\")\n",
        "print(gold_image_filenames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwp5jg-vBFMu",
        "outputId": "52de6f22-2cf2-4dad-c7ae-07bb2a2d82b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: torch.Size([1, 512, 512]), dtype: torch.float32\n",
            "Gold standard shape: torch.Size([1, 512, 512]), dtype: torch.float32\n"
          ]
        }
      ],
      "source": [
        "image, gold_standard, _, _ = train_dataset[0]\n",
        "print(f\"Image shape: {image.shape}, dtype: {image.dtype}\")\n",
        "print(f\"Gold standard shape: {gold_standard.shape}, dtype: {gold_standard.dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UegWH9c9Bqpm",
        "outputId": "d36489d2-f242-463c-bf0b-f62ab2323d76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in CT image: tensor([0.0000, 0.0039, 0.0078, 0.0118, 0.0157, 0.0196, 0.0235, 0.0275, 0.0314,\n",
            "        0.0353, 0.0392, 0.0431, 0.0471, 0.0510, 0.0549, 0.0588, 0.0627, 0.0667,\n",
            "        0.0706, 0.0745, 0.0784, 0.0824, 0.0863, 0.0902, 0.0941, 0.0980, 0.1020,\n",
            "        0.1059, 0.1098, 0.1137, 0.1176, 0.1216, 0.1255, 0.1294, 0.1333, 0.1373,\n",
            "        0.1412, 0.1451, 0.1490, 0.1529, 0.1569, 0.1608, 0.1647, 0.1686, 0.1725,\n",
            "        0.1765, 0.1804, 0.1843, 0.1882, 0.1922, 0.1961, 0.2000, 0.2039, 0.2078,\n",
            "        0.2118, 0.2157, 0.2196, 0.2235, 0.2275, 0.2314, 0.2353, 0.2392, 0.2431,\n",
            "        0.2471, 0.2510, 0.2549, 0.2588, 0.2627, 0.2667, 0.2706, 0.2745, 0.2784,\n",
            "        0.2824, 0.2863, 0.2902, 0.2941, 0.2980, 0.3020, 0.3059, 0.3098, 0.3137,\n",
            "        0.3176, 0.3216, 0.3255, 0.3294, 0.3333, 0.3373, 0.3412, 0.3451, 0.3490,\n",
            "        0.3529, 0.3569, 0.3608, 0.3647, 0.3686, 0.3725, 0.3765, 0.3804, 0.3843,\n",
            "        0.3882, 0.3922, 0.3961, 0.4000, 0.4039, 0.4078, 0.4118, 0.4157, 0.4196,\n",
            "        0.4235, 0.4275, 0.4314, 0.4353, 0.4392, 0.4431, 0.4471, 0.4510, 0.4549,\n",
            "        0.4588, 0.4627, 0.4667, 0.4706, 0.4745, 0.4784, 0.4824, 0.4863, 0.4902,\n",
            "        0.4941, 0.4980, 0.5020, 0.5059, 0.5098, 0.5137, 0.5176, 0.5216, 0.5255,\n",
            "        0.5294, 0.5333, 0.5373, 0.5412, 0.5451, 0.5490, 0.5529, 0.5569, 0.5608,\n",
            "        0.5647, 0.5686, 0.5725, 0.5765, 0.5804, 0.5843, 0.5882, 0.5922, 0.5961,\n",
            "        0.6000, 0.6039, 0.6078, 0.6118, 0.6157, 0.6196, 0.6235, 0.6275, 0.6314,\n",
            "        0.6353, 0.6392, 0.6431, 0.6471, 0.6510, 0.6549, 0.6588, 0.6627, 0.6667,\n",
            "        0.6706, 0.6745, 0.6784, 0.6824, 0.6863, 0.6902, 0.6941, 0.6980, 0.7020,\n",
            "        0.7059, 0.7098, 0.7137, 0.7176, 0.7216, 0.7255, 0.7294, 0.7333, 0.7373,\n",
            "        0.7412, 0.7451, 0.7490, 0.7529, 0.7569, 0.7608, 0.7647, 0.7686, 0.7725,\n",
            "        0.7765, 0.7804, 0.7843, 0.7882, 0.7922, 0.7961, 0.8000, 0.8039, 0.8078,\n",
            "        0.8118, 0.8157, 0.8196, 0.8235, 0.8275, 0.8314, 0.8353, 0.8392, 0.8431,\n",
            "        0.8471, 0.8510, 0.8549, 0.8588, 0.8627, 0.8667, 0.8706, 0.8745, 0.8824,\n",
            "        0.8863, 0.8902, 0.8941, 0.8980, 0.9020, 0.9059, 0.9098, 0.9137, 0.9176,\n",
            "        0.9216, 0.9255, 0.9294, 0.9333, 0.9373, 0.9412, 0.9451, 0.9569, 0.9647,\n",
            "        0.9725, 0.9804, 0.9922])\n",
            "Unique values in gold_standard: tensor([0., 1.])\n"
          ]
        }
      ],
      "source": [
        "image, gold_standard, _, _ = train_dataset[55]\n",
        "\n",
        "image_unique_values = torch.unique(image)\n",
        "gold_unique_values = torch.unique(gold_standard)\n",
        "\n",
        "print(f\"Unique values in CT image: {image_unique_values}\")\n",
        "print(f\"Unique values in gold_standard: {gold_unique_values}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow_mydata(image, gold_standard):\n",
        "    image_np = image.squeeze().numpy()\n",
        "    gold_standard_np = gold_standard.squeeze().numpy()\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "    axes[0].imshow(image_np, cmap='gray')\n",
        "    axes[0].set_title(f'Normalized CT Image')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(gold_standard_np, cmap='gray')\n",
        "    axes[1].set_title(f'Gold Standard')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "image, gold_standard, _, _ = train_dataset[1]\n",
        "imshow_mydata(image, gold_standard)"
      ],
      "metadata": {
        "id": "3Bjx0a5PGAPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LVLrXlA3-Ab"
      },
      "outputs": [],
      "source": [
        "batch_size = 2\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u99tKK4ZEdW-"
      },
      "outputs": [],
      "source": [
        "# https://github.com/milesial/Pytorch-UNet/blob/master/unet/unet_parts.py\n",
        "# https://github.com/Miltos-90/UNet_Biomedical_Image_Segmentation/blob/main/Unet_main.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIpHVo4HpcMF",
        "outputId": "3a065166-8531-4645-b08f-0db6e2a2fb86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x1:  torch.Size([1, 64, 568, 568])\n",
            "x2:  torch.Size([1, 128, 280, 280])\n",
            "x3:  torch.Size([1, 256, 136, 136])\n",
            "x4:  torch.Size([1, 512, 64, 64])\n",
            "x5:  torch.Size([1, 1024, 28, 28])\n",
            "x:  torch.Size([1, 512, 52, 52])\n",
            "x:  torch.Size([1, 256, 100, 100])\n",
            "x:  torch.Size([1, 128, 196, 196])\n",
            "x:  torch.Size([1, 64, 388, 388])\n",
            "Output dimensions: torch.Size([1, 2, 388, 388])\n"
          ]
        }
      ],
      "source": [
        "# ORIGINAL ARCH IN THE UNET PAPER\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3),\n",
        "                                         nn.ReLU(inplace=True),\n",
        "                                         nn.Conv2d(out_channels, out_channels, kernel_size=3),\n",
        "                                         nn.ReLU(inplace=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(nn.MaxPool2d(2),\n",
        "                                          DoubleConv(in_channels, out_channels))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "        x2 = x2[:, :, diffY // 2:x2.size()[2] - diffY // 2, diffX // 2:x2.size()[3] - diffX // 2]\n",
        "\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 1024)\n",
        "        self.up1 = Up(1024, 512)\n",
        "        self.up2 = Up(512, 256)\n",
        "        self.up3 = Up(256, 128)\n",
        "        self.up4 = Up(128, 64)\n",
        "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        print(\"x1: \", x1.shape)\n",
        "        x2 = self.down1(x1)\n",
        "        print(\"x2: \", x2.shape)\n",
        "        x3 = self.down2(x2)\n",
        "        print(\"x3: \", x3.shape)\n",
        "        x4 = self.down3(x3)\n",
        "        print(\"x4: \", x4.shape)\n",
        "        x5 = self.down4(x4)\n",
        "        print(\"x5: \", x5.shape)\n",
        "        x = self.up1(x5, x4)\n",
        "        print(\"x: \", x.shape)\n",
        "        x = self.up2(x, x3)\n",
        "        print(\"x: \", x.shape)\n",
        "        x = self.up3(x, x2)\n",
        "        print(\"x: \", x.shape)\n",
        "        x = self.up4(x, x1)\n",
        "        print(\"x: \", x.shape)\n",
        "        logits = self.outc(x)\n",
        "        return logits\n",
        "\n",
        "model = UNet(n_channels=1, n_classes=2)\n",
        "input = torch.rand(1, 1, 572, 572)\n",
        "output = model(input)\n",
        "print(\"Output dimensions:\", output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Default UNet with Batch Normalization\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                                         nn.BatchNorm2d(out_channels),\n",
        "                                         nn.ReLU(inplace=True),\n",
        "                                         nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "                                         nn.BatchNorm2d(out_channels),\n",
        "                                         nn.ReLU(inplace=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(nn.MaxPool2d(2),\n",
        "                                          DoubleConv(in_channels, out_channels))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "        x2 = x2[:, :, diffY // 2:x2.size()[2] - diffY // 2, diffX // 2:x2.size()[3] - diffX // 2]\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class default_unet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(default_unet, self).__init__()\n",
        "        self.inc = DoubleConv(n_channels, 16)\n",
        "        self.down1 = Down(16, 32)\n",
        "        self.down2 = Down(32, 64)\n",
        "        self.down3 = Down(64, 128)\n",
        "        self.down4 = Down(128, 256)\n",
        "        self.up1 = Up(256, 128)\n",
        "        self.up2 = Up(128, 64)\n",
        "        self.up3 = Up(64, 32)\n",
        "        self.up4 = Up(32, 16)\n",
        "        self.outc = nn.Conv2d(16, n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits\n",
        "\n",
        "model = default_unet(n_channels=1, n_classes=1)\n",
        "input = torch.rand(1, 1, 512, 512)\n",
        "output = model(input)\n",
        "print(\"Output dimensions:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbUCSqOJAAt9",
        "outputId": "2555e6a5-27da-4318-9fbe-dcce57d3436f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output dimensions: torch.Size([1, 1, 512, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, num_epochs=30, patience=5, threshold = 0.5):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    best_model_wts = None\n",
        "    best_f1 = 0.0\n",
        "    best_epoch = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    all_train_losses = []\n",
        "    all_val_losses = []\n",
        "    all_train_precisions = []\n",
        "    all_val_precisions = []\n",
        "    all_train_recalls = []\n",
        "    all_val_recalls = []\n",
        "    all_train_f1s = []\n",
        "    all_val_f1s = []\n",
        "\n",
        "    precision_metric = BinaryPrecision(threshold).to(device)\n",
        "    recall_metric = BinaryRecall(threshold).to(device)\n",
        "    f1_metric = BinaryF1Score(threshold).to(device)\n",
        "\n",
        "    print(\"Training started:\\n\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        epoch_train_precisions, epoch_train_recalls, epoch_train_f1s = [], [], []\n",
        "\n",
        "        for images, masks, _, _ in tqdm(train_loader):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            train_outputs = torch.sigmoid(outputs)\n",
        "            train_preds = train_outputs > threshold\n",
        "\n",
        "            # Calculate and accumulate metrics\n",
        "            precision = precision_metric(train_preds, masks)\n",
        "            recall = recall_metric(train_preds, masks)\n",
        "            f1_score = f1_metric(train_preds, masks)\n",
        "\n",
        "            epoch_train_precisions.append(precision.item())\n",
        "            epoch_train_recalls.append(recall.item())\n",
        "            epoch_train_f1s.append(f1_score.item())\n",
        "\n",
        "        # Compute average metrics for training\n",
        "        avg_train_precision = sum(epoch_train_precisions) / len(epoch_train_precisions)\n",
        "        avg_train_recall = sum(epoch_train_recalls) / len(epoch_train_recalls)\n",
        "        avg_train_f1 = sum(epoch_train_f1s) / len(epoch_train_f1s)\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # Store the metrics for later averaging\n",
        "        all_train_losses.append(avg_train_loss)\n",
        "        all_train_precisions.append(avg_train_precision)\n",
        "        all_train_recalls.append(avg_train_recall)\n",
        "        all_train_f1s.append(avg_train_f1)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} Train Loss: {avg_train_loss:.4f}\")\n",
        "        print(f\"Train Pixel-level Precision: {avg_train_precision:.4f}\")\n",
        "        print(f\"Train Recall: {avg_train_recall:.4f}\")\n",
        "        print(f\"Train F score: {avg_train_f1:.4f}\\n\")\n",
        "\n",
        "        precision_metric.reset()\n",
        "        recall_metric.reset()\n",
        "        f1_metric.reset()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        epoch_val_precisions, epoch_val_recalls, epoch_val_f1s = [], [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for val_images, val_masks, _, _ in val_loader:\n",
        "                val_images = val_images.to(device)\n",
        "                val_masks = val_masks.to(device)\n",
        "\n",
        "                val_outputs = model(val_images)\n",
        "                loss = criterion(val_outputs, val_masks)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                val_outputs = torch.sigmoid(val_outputs)\n",
        "                val_preds = val_outputs > threshold\n",
        "\n",
        "                # Calculate and accumulate metrics\n",
        "                precision = precision_metric(val_preds, val_masks)\n",
        "                recall = recall_metric(val_preds, val_masks)\n",
        "                f1_score = f1_metric(val_preds, val_masks)\n",
        "\n",
        "                epoch_val_precisions.append(precision.item())\n",
        "                epoch_val_recalls.append(recall.item())\n",
        "                epoch_val_f1s.append(f1_score.item())\n",
        "\n",
        "        # Compute average metrics for validation\n",
        "        avg_val_precision = sum(epoch_val_precisions) / len(epoch_val_precisions)\n",
        "        avg_val_recall = sum(epoch_val_recalls) / len(epoch_val_recalls)\n",
        "        avg_val_f1 = sum(epoch_val_f1s) / len(epoch_val_f1s)\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        # Store the metrics for later averaging\n",
        "        all_val_losses.append(avg_val_loss)\n",
        "        all_val_precisions.append(avg_val_precision)\n",
        "        all_val_recalls.append(avg_val_recall)\n",
        "        all_val_f1s.append(avg_val_f1)\n",
        "\n",
        "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "        print(f\"Validation Pixel-level Precision: {avg_val_precision:.4f}\")\n",
        "        print(f\"Validation Recall: {avg_val_recall:.4f}\")\n",
        "        print(f\"Validation F score: {avg_val_f1:.4f}\\n\")\n",
        "\n",
        "        precision_metric.reset()\n",
        "        recall_metric.reset()\n",
        "        f1_metric.reset()\n",
        "\n",
        "        # Where to save the model\n",
        "        if avg_val_f1 > best_f1:\n",
        "            best_f1 = avg_val_f1\n",
        "            best_model_wts = model.state_dict()\n",
        "            best_epoch = epoch + 1\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step(avg_val_loss)\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            print('Early stopping triggered')\n",
        "            break\n",
        "\n",
        "    if best_model_wts is not None:\n",
        "        model.load_state_dict(best_model_wts)\n",
        "\n",
        "    # Calculate average metrics over all epochs\n",
        "    final_avg_train_loss = sum(all_train_losses) / len(all_train_losses)\n",
        "    final_avg_train_precision = sum(all_train_precisions) / len(all_train_precisions)\n",
        "    final_avg_train_recall = sum(all_train_recalls) / len(all_train_recalls)\n",
        "    final_avg_train_f1 = sum(all_train_f1s) / len(all_train_f1s)\n",
        "\n",
        "    final_avg_val_loss = sum(all_val_losses) / len(all_val_losses)\n",
        "    final_avg_val_precision = sum(all_val_precisions) / len(all_val_precisions)\n",
        "    final_avg_val_recall = sum(all_val_recalls) / len(all_val_recalls)\n",
        "    final_avg_val_f1 = sum(all_val_f1s) / len(all_val_f1s)\n",
        "\n",
        "    # Calculate standard deviations\n",
        "    std_train_loss = statistics.stdev(all_train_losses)\n",
        "    std_train_precision = statistics.stdev(all_train_precisions)\n",
        "    std_train_recall = statistics.stdev(all_train_recalls)\n",
        "    std_train_f1 = statistics.stdev(all_train_f1s)\n",
        "\n",
        "    std_val_loss = statistics.stdev(all_val_losses)\n",
        "    std_val_precision = statistics.stdev(all_val_precisions)\n",
        "    std_val_recall = statistics.stdev(all_val_recalls)\n",
        "    std_val_f1 = statistics.stdev(all_val_f1s)\n",
        "\n",
        "    print(\"Training ended...\\n\")\n",
        "    print(f\"Train Average Loss: {final_avg_train_loss:.4f}, Std: {std_train_loss:.4f}\")\n",
        "    print(f\"Train Average Pixel-level Precision: {final_avg_train_precision:.4f}, Std: {std_train_precision:.4f}\")\n",
        "    print(f\"Train Average Recall: {final_avg_train_recall:.4f}, Std: {std_train_recall:.4f}\")\n",
        "    print(f\"Train Average F score: {final_avg_train_f1:.4f}, Std: {std_train_f1:.4f}\\n\")\n",
        "    print(f\"Validation Average Loss: {final_avg_val_loss:.4f}, Std: {std_val_loss:.4f}\")\n",
        "    print(f\"Validation Average Pixel-level Precision: {final_avg_val_precision:.4f}, Std: {std_val_precision:.4f}\")\n",
        "    print(f\"Validation Average Recall: {final_avg_val_recall:.4f}, Std: {std_val_recall:.4f}\")\n",
        "    print(f\"Validation Average F score: {final_avg_val_f1:.4f}, Std: {std_val_f1:.4f}\")\n",
        "\n",
        "    if best_model_wts is not None:\n",
        "        print(f\"\\nBest model saved from epoch {best_epoch} with Validation F score: {best_f1:.4f}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "ULDsf69PqWJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, masks, _, _ in train_loader:\n",
        "    print(\"Inputs shape:\", inputs.shape)\n",
        "    print(\"Gold standard shape:\", masks.shape)\n",
        "    print(\"Input min and max values:\", inputs.min().item(), inputs.max().item())\n",
        "    print(\"Gold standard unique values:\", np.unique(masks.numpy()))\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daUKfaBDTJJX",
        "outputId": "9322d6d7-deea-4f6c-a30e-07a5513d0bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs shape: torch.Size([2, 1, 512, 512])\n",
            "Gold standard shape: torch.Size([2, 1, 512, 512])\n",
            "Input min and max values: 0.0 1.0\n",
            "Gold standard unique values: [0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbGqfkdLKr3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a941f6b-9d5d-431a-dac9-2b0879cab14d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:04<00:00, 14.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 Train Loss: 0.3894\n",
            "Train Pixel-level Precision: 0.1997\n",
            "Train Recall: 0.0686\n",
            "Train F score: 0.0981\n",
            "\n",
            "Validation Loss: 0.2993\n",
            "Validation Pixel-level Precision: 0.0000\n",
            "Validation Recall: 0.0000\n",
            "Validation F score: 0.0000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:04<00:00, 14.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 Train Loss: 0.3101\n",
            "Train Pixel-level Precision: 0.6758\n",
            "Train Recall: 0.3898\n",
            "Train F score: 0.4839\n",
            "\n",
            "Validation Loss: 0.4179\n",
            "Validation Pixel-level Precision: 0.2179\n",
            "Validation Recall: 0.9257\n",
            "Validation F score: 0.3438\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:04<00:00, 14.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 Train Loss: 0.2623\n",
            "Train Pixel-level Precision: 0.8076\n",
            "Train Recall: 0.6496\n",
            "Train F score: 0.7111\n",
            "\n",
            "Validation Loss: 0.4091\n",
            "Validation Pixel-level Precision: 0.2710\n",
            "Validation Recall: 0.9614\n",
            "Validation F score: 0.4086\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:04<00:00, 14.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 Train Loss: 0.2286\n",
            "Train Pixel-level Precision: 0.8685\n",
            "Train Recall: 0.7393\n",
            "Train F score: 0.7889\n",
            "\n",
            "Validation Loss: 0.5525\n",
            "Validation Pixel-level Precision: 0.1765\n",
            "Validation Recall: 0.9979\n",
            "Validation F score: 0.2948\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:04<00:00, 14.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20 Train Loss: 0.2034\n",
            "Train Pixel-level Precision: 0.9121\n",
            "Train Recall: 0.7805\n",
            "Train F score: 0.8359\n",
            "\n",
            "Validation Loss: 0.2949\n",
            "Validation Pixel-level Precision: 0.4494\n",
            "Validation Recall: 0.9643\n",
            "Validation F score: 0.5738\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:04<00:00, 14.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20 Train Loss: 0.1884\n",
            "Train Pixel-level Precision: 0.9293\n",
            "Train Recall: 0.7884\n",
            "Train F score: 0.8494\n",
            "\n",
            "Validation Loss: 0.2652\n",
            "Validation Pixel-level Precision: 0.4809\n",
            "Validation Recall: 0.9253\n",
            "Validation F score: 0.5934\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:04<00:00, 14.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20 Train Loss: 0.1780\n",
            "Train Pixel-level Precision: 0.9411\n",
            "Train Recall: 0.7826\n",
            "Train F score: 0.8506\n",
            "\n",
            "Validation Loss: 0.1895\n",
            "Validation Pixel-level Precision: 0.5528\n",
            "Validation Recall: 0.7458\n",
            "Validation F score: 0.6219\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:04<00:00, 14.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20 Train Loss: 0.1674\n",
            "Train Pixel-level Precision: 0.9519\n",
            "Train Recall: 0.8078\n",
            "Train F score: 0.8713\n",
            "\n",
            "Validation Loss: 0.2730\n",
            "Validation Pixel-level Precision: 0.3732\n",
            "Validation Recall: 0.9270\n",
            "Validation F score: 0.5121\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:04<00:00, 14.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20 Train Loss: 0.1605\n",
            "Train Pixel-level Precision: 0.9516\n",
            "Train Recall: 0.8062\n",
            "Train F score: 0.8690\n",
            "\n",
            "Validation Loss: 0.2285\n",
            "Validation Pixel-level Precision: 0.4506\n",
            "Validation Recall: 0.9079\n",
            "Validation F score: 0.5753\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:04<00:00, 14.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20 Train Loss: 0.1523\n",
            "Train Pixel-level Precision: 0.9635\n",
            "Train Recall: 0.8146\n",
            "Train F score: 0.8813\n",
            "\n",
            "Validation Loss: 0.2357\n",
            "Validation Pixel-level Precision: 0.4291\n",
            "Validation Recall: 0.9029\n",
            "Validation F score: 0.5553\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:04<00:00, 14.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20 Train Loss: 0.1459\n",
            "Train Pixel-level Precision: 0.9662\n",
            "Train Recall: 0.8231\n",
            "Train F score: 0.8868\n",
            "\n",
            "Validation Loss: 0.1694\n",
            "Validation Pixel-level Precision: 0.5153\n",
            "Validation Recall: 0.6434\n",
            "Validation F score: 0.5592\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:04<00:00, 14.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20 Train Loss: 0.1405\n",
            "Train Pixel-level Precision: 0.9628\n",
            "Train Recall: 0.8189\n",
            "Train F score: 0.8824\n",
            "\n",
            "Validation Loss: 0.2419\n",
            "Validation Pixel-level Precision: 0.3829\n",
            "Validation Recall: 0.9198\n",
            "Validation F score: 0.5162\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:04<00:00, 14.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20 Train Loss: 0.1338\n",
            "Train Pixel-level Precision: 0.9684\n",
            "Train Recall: 0.8365\n",
            "Train F score: 0.8966\n",
            "\n",
            "Validation Loss: 0.1971\n",
            "Validation Pixel-level Precision: 0.4446\n",
            "Validation Recall: 0.8303\n",
            "Validation F score: 0.5601\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:04<00:00, 14.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20 Train Loss: 0.1283\n",
            "Train Pixel-level Precision: 0.9732\n",
            "Train Recall: 0.8383\n",
            "Train F score: 0.9000\n",
            "\n",
            "Validation Loss: 0.1639\n",
            "Validation Pixel-level Precision: 0.5460\n",
            "Validation Recall: 0.8204\n",
            "Validation F score: 0.6268\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:04<00:00, 14.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20 Train Loss: 0.1245\n",
            "Train Pixel-level Precision: 0.9709\n",
            "Train Recall: 0.8244\n",
            "Train F score: 0.8904\n",
            "\n",
            "Validation Loss: 0.1804\n",
            "Validation Pixel-level Precision: 0.5084\n",
            "Validation Recall: 0.8829\n",
            "Validation F score: 0.6105\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:04<00:00, 14.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/20 Train Loss: 0.1197\n",
            "Train Pixel-level Precision: 0.9663\n",
            "Train Recall: 0.8335\n",
            "Train F score: 0.8937\n",
            "\n",
            "Validation Loss: 0.1706\n",
            "Validation Pixel-level Precision: 0.5178\n",
            "Validation Recall: 0.8263\n",
            "Validation F score: 0.6040\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:04<00:00, 14.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/20 Train Loss: 0.1153\n",
            "Train Pixel-level Precision: 0.9636\n",
            "Train Recall: 0.8401\n",
            "Train F score: 0.8954\n",
            "\n",
            "Validation Loss: 0.1626\n",
            "Validation Pixel-level Precision: 0.5325\n",
            "Validation Recall: 0.7981\n",
            "Validation F score: 0.6110\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:04<00:00, 14.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20 Train Loss: 0.1118\n",
            "Train Pixel-level Precision: 0.9678\n",
            "Train Recall: 0.8324\n",
            "Train F score: 0.8938\n",
            "\n",
            "Validation Loss: 0.1230\n",
            "Validation Pixel-level Precision: 0.5882\n",
            "Validation Recall: 0.5747\n",
            "Validation F score: 0.5615\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:04<00:00, 14.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/20 Train Loss: 0.1094\n",
            "Train Pixel-level Precision: 0.9609\n",
            "Train Recall: 0.8155\n",
            "Train F score: 0.8790\n",
            "\n",
            "Validation Loss: 0.1193\n",
            "Validation Pixel-level Precision: 0.5760\n",
            "Validation Recall: 0.6123\n",
            "Validation F score: 0.5849\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:04<00:00, 14.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20 Train Loss: 0.1031\n",
            "Train Pixel-level Precision: 0.9702\n",
            "Train Recall: 0.8444\n",
            "Train F score: 0.9016\n",
            "\n",
            "Validation Loss: 0.1409\n",
            "Validation Pixel-level Precision: 0.5328\n",
            "Validation Recall: 0.7913\n",
            "Validation F score: 0.6163\n",
            "\n",
            "Training ended...\n",
            "\n",
            "Train Average Loss: 0.1736, Std: 0.0743\n",
            "Train Average Pixel-level Precision: 0.8936, Std: 0.1787\n",
            "Train Average Recall: 0.7467, Std: 0.1898\n",
            "Train Average F score: 0.8080, Std: 0.1929\n",
            "\n",
            "Validation Average Loss: 0.2417, Std: 0.1109\n",
            "Validation Average Pixel-level Precision: 0.4273, Std: 0.1531\n",
            "Validation Average Recall: 0.7979, Std: 0.2223\n",
            "Validation Average F score: 0.5165, Std: 0.1523\n",
            "\n",
            "Best model saved from epoch 14 with Validation F score: 0.6268\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 20\n",
        "patience = 5\n",
        "threshold = 0.75\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.BatchNorm2d):\n",
        "        nn.init.ones_(m.weight)\n",
        "        nn.init.zeros_(m.bias)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "p = 0.5\n",
        "model = default_unet(n_channels=1, n_classes=1, dropout_prob=p).to(device)\n",
        "\n",
        "model.apply(init_weights)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999), eps=1e-8)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=patience)\n",
        "\n",
        "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=scheduler, num_epochs=num_epochs, patience=patience, threshold=threshold)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = '.../path/...'\n",
        "torch.save(trained_model.state_dict(), model_save_path)"
      ],
      "metadata": {
        "id": "eOvRAhfDw3Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD AND TEST MODE\n",
        "\n",
        "model_path = '.../path/...'\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "trained_model = default_unet(n_channels=1, n_classes=1).to(device)\n",
        "\n",
        "trained_model.load_state_dict(torch.load(model_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODcn_lgJhtvj",
        "outputId": "61e834b9-0c08-428e-eb20-af3744436367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, test_loader, threshold=0.2):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "\n",
        "    precision_metric = BinaryPrecision().to(device)\n",
        "    recall_metric = BinaryRecall().to(device)\n",
        "    f1_metric = BinaryF1Score().to(device)\n",
        "\n",
        "    all_precisions = []\n",
        "    all_recalls = []\n",
        "    all_f1s = []\n",
        "\n",
        "    good_results = []\n",
        "    acceptable_results = []\n",
        "    problematic_results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, masks, image_names, mask_names) in enumerate(test_loader):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            outputs = torch.sigmoid(outputs)\n",
        "            preds = outputs > threshold\n",
        "\n",
        "            for j in range(images.size(0)):\n",
        "                precision = precision_metric(preds[j], masks[j])\n",
        "                recall = recall_metric(preds[j], masks[j])\n",
        "                f1 = f1_metric(preds[j], masks[j])\n",
        "\n",
        "                print(f\"Image: {image_names[j]}, Precision: {precision.item():.4f}, Recall: {recall.item():.4f}, F1 Score: {f1.item():.4f}\")\n",
        "\n",
        "                all_precisions.append(precision.item())\n",
        "                all_recalls.append(recall.item())\n",
        "                all_f1s.append(f1.item())\n",
        "\n",
        "                # Good, acceptable, problematic threshold\n",
        "                result = (images[j].cpu(), masks[j].cpu(), preds[j].cpu(), image_names[j], mask_names[j], f1.item())\n",
        "                if f1 > 0.85:\n",
        "                    good_results.append(result)\n",
        "                elif 0.75 <= f1 <= 0.85:\n",
        "                    acceptable_results.append(result)\n",
        "                else:\n",
        "                    problematic_results.append(result)\n",
        "\n",
        "    avg_precision = sum(all_precisions) / len(all_precisions)\n",
        "    avg_recall = sum(all_recalls) / len(all_recalls)\n",
        "    avg_f1 = sum(all_f1s) / len(all_f1s)\n",
        "\n",
        "    std_test_precision = statistics.stdev(all_precisions)\n",
        "    std_test_recall = statistics.stdev(all_recalls)\n",
        "    std_test_f1 = statistics.stdev(all_f1s)\n",
        "\n",
        "    print(f\"Test Average Precision: {avg_precision:.4f}, Std: {std_test_precision:.4f}\")\n",
        "    print(f\"Test Average Recall: {avg_recall:.4f}, Std: {std_test_recall:.4f}\")\n",
        "    print(f\"Test Average F1 Score: {avg_f1:.4f}, Std: {std_test_f1:.4f}\")\n",
        "\n",
        "    # pick best ones\n",
        "    good_results = sorted(good_results, key=lambda x: x[5], reverse=True)[:2]\n",
        "    acceptable_results = sorted(acceptable_results, key=lambda x: x[5], reverse=True)[:2]\n",
        "    problematic_results = sorted(problematic_results, key=lambda x: x[5], reverse=True)[:2]\n",
        "\n",
        "    def gold_pred_plot(results, title):\n",
        "        for idx, (image, mask, pred, img_name, mask_name, f1_score) in enumerate(results):\n",
        "            plt.figure(figsize=(15, 5))\n",
        "            plt.subplot(1, 3, 1)\n",
        "            plt.imshow(image.squeeze(0), cmap='gray')\n",
        "            plt.title(f'Original Image: {img_name}')\n",
        "\n",
        "            plt.subplot(1, 3, 2)\n",
        "            plt.imshow(mask.squeeze(0), cmap='gray')\n",
        "            plt.title(f'Gold Standard: {mask_name}')\n",
        "\n",
        "            plt.subplot(1, 3, 3)\n",
        "            plt.imshow(pred.squeeze(0), cmap='gray')\n",
        "            plt.title('Predicted Segmentation')\n",
        "\n",
        "            plt.suptitle(f'{title} {idx+1} Segmentation - F1 Score: {f1_score:.4f}')\n",
        "            plt.show()\n",
        "\n",
        "    gold_pred_plot(good_results, 'GOOD')\n",
        "    gold_pred_plot(acceptable_results, 'Acceptable')\n",
        "    gold_pred_plot(problematic_results, 'Problematic')\n",
        "\n",
        "test_model(trained_model, test_loader, threshold=threshold)"
      ],
      "metadata": {
        "id": "r8mFPUA2Hsbn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}