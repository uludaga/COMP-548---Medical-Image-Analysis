{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNTMl0I5rRUl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceb42fde-e219-4945-c80a-822c8087bd0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "OYalsJmn_6_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40bd694f-ad3d-4c2b-f7ad-da2ff7f2ab0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.3.0+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torchmetrics-1.4.0.post0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize, ToPILImage\n",
        "from torchvision.io import read_image\n",
        "from torchvision import transforms\n",
        "from torchmetrics.classification import BinaryPrecision, BinaryRecall, BinaryF1Score\n",
        "import statistics\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "import torch.utils.data\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, Subset\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import copy\n",
        "import multiprocessing as mp\n",
        "from tqdm import tqdm\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import statistics\n",
        "import time\n",
        "\n",
        "import glob\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "\n",
        "from torchvision.models import resnet18, ResNet18_Weights"
      ],
      "metadata": {
        "id": "tGucad0D_-sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "mp.set_start_method('spawn', force=True)"
      ],
      "metadata": {
        "id": "iwnEUomBABpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im_path = '.../path/...'\n",
        "im = Image.open(im_path)\n",
        "image_array = np.array(im)\n",
        "print(image_array.max())\n",
        "print(image_array.min())\n",
        "print(im.mode)\n",
        "_, file_extension = os.path.splitext(im_path)\n",
        "print(file_extension)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHkozMs6AEOq",
        "outputId": "e86ef036-49ff-47dc-cade-f8e18c40e8c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "255\n",
            "0\n",
            "L\n",
            ".png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "im_path = '.../path/...'\n",
        "im = Image.open(im_path)\n",
        "image_array = np.array(im)\n",
        "print(image_array.max())\n",
        "print(image_array.min())\n",
        "print(im.mode)\n",
        "_, file_extension = os.path.splitext(im_path)\n",
        "print(file_extension)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHwut7oIAlna",
        "outputId": "4bf20ebe-0591-4729-db06-dbcadacd9286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "255\n",
            "0\n",
            "L\n",
            ".png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = [0 if \"Normal\" in path else 1 for path in image_paths]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert('L')\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "bvOzambVAt7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_images_pattern = '.../path/...'\n",
        "pneumonia_images_pattern = '.../path/...'\n",
        "\n",
        "normal_image_paths = glob.glob(normal_images_pattern)\n",
        "pneumonia_image_paths = glob.glob(pneumonia_images_pattern)\n",
        "\n",
        "image_paths = normal_image_paths + pneumonia_image_paths"
      ],
      "metadata": {
        "id": "snqbeu-7FygX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.Resize((256, 256)),\n",
        "                               transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "WxgCmSQ-DBjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MyDataset(image_paths, transform=transform)\n",
        "\n",
        "total_count = len(dataset)\n",
        "train_count = int(0.7 * total_count)\n",
        "valid_count = int(0.15 * total_count)\n",
        "test_count = total_count - train_count - valid_count\n",
        "\n",
        "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_count, valid_count, test_count])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=10, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=True)\n",
        "\n",
        "print(\"Total images:\", total_count)\n",
        "print(\"Training set size:\", len(train_dataset))\n",
        "print(\"Validation set size:\", len(valid_dataset))\n",
        "print(\"Test set size:\", len(test_dataset))"
      ],
      "metadata": {
        "id": "pr_vNeAeCaCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "991ea95c-bc9b-4aaa-8ee5-7c0c234a73a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 11537\n",
            "Training set size: 8075\n",
            "Validation set size: 1730\n",
            "Test set size: 1732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_label_0 = 0\n",
        "count_label_1 = 0\n",
        "\n",
        "for idx in train_dataset.indices:\n",
        "    label = dataset.labels[idx]\n",
        "    if label == 0:\n",
        "        count_label_0 += 1\n",
        "    elif label == 1:\n",
        "        count_label_1 += 1\n",
        "\n",
        "print(f\"Count of Label 0 (Normal): {count_label_0}\")\n",
        "print(f\"Count of Label 1 (Viral Pneumonia): {count_label_1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW3PJ2aKK8IH",
        "outputId": "b51f3a9f-20df-45fb-f0b7-f173f251db55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of Label 0 (Normal): 7132\n",
            "Count of Label 1 (Viral Pneumonia): 943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Total samples\n",
        "total_samples = count_label_0 + count_label_1\n",
        "\n",
        "# Calculate weights for each class\n",
        "weight_for_0 = total_samples / (2 * count_label_0)\n",
        "weight_for_1 = total_samples / (2 * count_label_1)\n",
        "\n",
        "# Create a tensor of weights for use with the loss function\n",
        "class_weights = torch.tensor([weight_for_0, weight_for_1], dtype=torch.float32)"
      ],
      "metadata": {
        "id": "lpvw2vGCK-PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMqbQuemLK6t",
        "outputId": "8044e4ae-f591-4686-c990-7f65cfc3eab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5675, 4.2014])"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_label_0 = sum(1 for label in dataset.labels if label == 0)\n",
        "original_label_1 = sum(1 for label in dataset.labels if label == 1)\n",
        "print(f\"Original Count of Label 0: {original_label_0}\")\n",
        "print(f\"Original Count of Label 1: {original_label_1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EYzYVAcSUdF",
        "outputId": "78e4ac3f-a64b-49ee-e140-94916c87e78e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Count of Label 0: 10192\n",
            "Original Count of Label 1: 1345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET\n",
        "# CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET\n",
        "# CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET\n",
        "# CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET\n",
        "# CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET CREATE SUBSET\n",
        "\n",
        "# Total indices and labels from the dataset\n",
        "total_indices = list(range(len(dataset)))\n",
        "total_labels = dataset.labels  # Ensure this attribute exists and correctly corresponds to the indices\n",
        "\n",
        "# Step 1: Use train_test_split to create a stratified subset of 1000 samples\n",
        "subset_indices, _ = train_test_split(total_indices, train_size=1000, stratify=total_labels, random_state=42)\n",
        "\n",
        "# Step 2: Use train_test_split again to split the subset into train, validation, and test sets\n",
        "train_indices, test_indices = train_test_split(subset_indices, test_size=0.3, stratify=[total_labels[i] for i in subset_indices], random_state=42)\n",
        "valid_indices, test_indices = train_test_split(test_indices, test_size=0.5, stratify=[total_labels[i] for i in test_indices], random_state=42)\n",
        "\n",
        "# Step 3: Create Subset instances for train, validation, and test sets\n",
        "train_dataset = Subset(dataset, train_indices)\n",
        "valid_dataset = Subset(dataset, valid_indices)\n",
        "test_dataset = Subset(dataset, test_indices)\n",
        "\n",
        "# Step 4: Create DataLoaders for each dataset\n",
        "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=10, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=True)\n",
        "\n",
        "# Print information about the splits\n",
        "print(\"Total images in subset:\", 1000)\n",
        "print(\"Training set size:\", len(train_dataset))\n",
        "print(\"Validation set size:\", len(valid_dataset))\n",
        "print(\"Test set size:\", len(test_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRBbVPdCQwiQ",
        "outputId": "1fc4b618-07f5-43c8-f603-2fd8b4c40144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images in subset: 1000\n",
            "Training set size: 700\n",
            "Validation set size: 150\n",
            "Test set size: 150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_label_0 = 0\n",
        "count_label_1 = 0\n",
        "\n",
        "for idx in train_dataset.indices:\n",
        "    label = dataset.labels[idx]\n",
        "    if label == 0:\n",
        "        count_label_0 += 1\n",
        "    elif label == 1:\n",
        "        count_label_1 += 1\n",
        "\n",
        "print(f\"Count of Label 0 (Normal): {count_label_0}\")\n",
        "print(f\"Count of Label 1 (Viral Pneumonia): {count_label_1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltDPYl1xRF_2",
        "outputId": "ce542453-be24-45a6-d04e-d5feaa55e688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of Label 0 (Normal): 618\n",
            "Count of Label 1 (Viral Pneumonia): 82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_samples = count_label_0 + count_label_1\n",
        "\n",
        "weight_for_0 = total_samples / (2 * count_label_0)\n",
        "weight_for_1 = total_samples / (2 * count_label_1)\n",
        "\n",
        "class_weights = torch.tensor([weight_for_0, weight_for_1], dtype=torch.float32)"
      ],
      "metadata": {
        "id": "UJpZiTOVejNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZynckeBekDP",
        "outputId": "1c56bb7a-6aba-4c6e-aa05-1b8725fe43d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5663, 4.2683])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = Compose([\n",
        "    Resize((256, 256)),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=[0.485], std=[0.229])])"
      ],
      "metadata": {
        "id": "jahiEuNAQ5Hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_choices = [0.001, 0.01, 0.05, 0.1]\n",
        "batch_size_choices = [8, 16, 32, 64]\n",
        "optimizer_choices = [optim.Adam, optim.SGD]\n",
        "\n",
        "num_search_iterations = 5"
      ],
      "metadata": {
        "id": "Y-1NAH50BnQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(lr, optimizer_cls):\n",
        "    model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, 2)\n",
        "    optimizer = optimizer_cls(model.parameters(), lr=lr)\n",
        "    return model, optimizer"
      ],
      "metadata": {
        "id": "cJTzRCZABosK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=10, patience=5):\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        true_labels = []\n",
        "        pred_labels = []\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            true_labels.extend(labels.tolist())\n",
        "            pred_labels.extend(predicted.tolist())\n",
        "\n",
        "        train_accuracy = accuracy_score(true_labels, pred_labels)\n",
        "        train_precision = precision_score(true_labels, pred_labels, zero_division=0)\n",
        "        train_recall = recall_score(true_labels, pred_labels)\n",
        "        train_f1 = f1_score(true_labels, pred_labels)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_true_labels = []\n",
        "        val_pred_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in valid_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_true_labels.extend(labels.tolist())\n",
        "                val_pred_labels.extend(predicted.tolist())\n",
        "\n",
        "        val_accuracy = accuracy_score(val_true_labels, val_pred_labels)\n",
        "        val_precision = precision_score(val_true_labels, val_pred_labels, zero_division=0)\n",
        "        val_recall = recall_score(val_true_labels, val_pred_labels)\n",
        "        val_f1 = f1_score(val_true_labels, val_pred_labels)\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        avg_val_loss = val_loss / len(valid_loader)\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_model = model\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                print('Early stopping!')\n",
        "                break\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
        "        print(f'Train - Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}')\n",
        "        print(f'Validation - Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}')\n",
        "\n",
        "    return best_model"
      ],
      "metadata": {
        "id": "RlWfqxw4BsTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(num_search_iterations):\n",
        "    lr = np.random.choice(lr_choices)\n",
        "    batch_size = np.random.choice(batch_size_choices)\n",
        "    optimizer_cls = np.random.choice(optimizer_choices)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    model, optimizer = create_model(lr, optimizer_cls)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(f\"Starting Trial {i+1}: Learning Rate = {lr}, Batch Size = {batch_size}, Optimizer = {optimizer_cls.__name__}\")\n",
        "    trained_model = train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr4JR2aMN0fh",
        "outputId": "cc0bbfd3-fe3d-4bb2-972a-266fcfbbf52e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Trial 1: Learning Rate = 0.1, Batch Size = 16, Optimizer = Adam\n",
            "Epoch 1, Train Loss: 1.5231, Val Loss: 957.4786\n",
            "Train - Accuracy: 0.8643, Precision: 0.0667, Recall: 0.0122, F1: 0.0206\n",
            "Validation - Accuracy: 0.8800, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "Epoch 2, Train Loss: 0.2602, Val Loss: 0.5198\n",
            "Train - Accuracy: 0.8843, Precision: 0.6667, Recall: 0.0244, F1: 0.0471\n",
            "Validation - Accuracy: 0.8800, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "Epoch 3, Train Loss: 0.2190, Val Loss: 0.1863\n",
            "Train - Accuracy: 0.8943, Precision: 0.5667, Recall: 0.4146, F1: 0.4789\n",
            "Validation - Accuracy: 0.8867, Precision: 0.5185, Recall: 0.7778, F1: 0.6222\n",
            "Epoch 4, Train Loss: 0.2013, Val Loss: 0.4935\n",
            "Train - Accuracy: 0.8914, Precision: 0.5469, Recall: 0.4268, F1: 0.4795\n",
            "Validation - Accuracy: 0.6667, Precision: 0.2647, Recall: 1.0000, F1: 0.4186\n",
            "Epoch 5, Train Loss: 0.2220, Val Loss: 0.3075\n",
            "Train - Accuracy: 0.9000, Precision: 0.5882, Recall: 0.4878, F1: 0.5333\n",
            "Validation - Accuracy: 0.8467, Precision: 0.4138, Recall: 0.6667, F1: 0.5106\n",
            "Epoch 6, Train Loss: 0.1855, Val Loss: 0.2393\n",
            "Train - Accuracy: 0.9086, Precision: 0.7045, Recall: 0.3780, F1: 0.4921\n",
            "Validation - Accuracy: 0.8733, Precision: 0.4800, Recall: 0.6667, F1: 0.5581\n",
            "Starting Trial 2: Learning Rate = 0.01, Batch Size = 16, Optimizer = SGD\n",
            "Epoch 1, Train Loss: 0.2281, Val Loss: 0.1792\n",
            "Train - Accuracy: 0.9129, Precision: 0.7692, Recall: 0.3659, F1: 0.4959\n",
            "Validation - Accuracy: 0.9533, Precision: 0.7619, Recall: 0.8889, F1: 0.8205\n",
            "Epoch 2, Train Loss: 0.0876, Val Loss: 0.0819\n",
            "Train - Accuracy: 0.9729, Precision: 0.9437, Recall: 0.8171, F1: 0.8758\n",
            "Validation - Accuracy: 0.9800, Precision: 0.9412, Recall: 0.8889, F1: 0.9143\n",
            "Epoch 3, Train Loss: 0.0330, Val Loss: 0.0332\n",
            "Train - Accuracy: 0.9929, Precision: 0.9873, Recall: 0.9512, F1: 0.9689\n",
            "Validation - Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
            "Epoch 4, Train Loss: 0.0146, Val Loss: 0.0406\n",
            "Train - Accuracy: 0.9986, Precision: 0.9880, Recall: 1.0000, F1: 0.9939\n",
            "Validation - Accuracy: 0.9867, Precision: 1.0000, Recall: 0.8889, F1: 0.9412\n",
            "Epoch 5, Train Loss: 0.0173, Val Loss: 0.0744\n",
            "Train - Accuracy: 0.9986, Precision: 1.0000, Recall: 0.9878, F1: 0.9939\n",
            "Validation - Accuracy: 0.9733, Precision: 0.8500, Recall: 0.9444, F1: 0.8947\n",
            "Epoch 6, Train Loss: 0.0109, Val Loss: 0.0346\n",
            "Train - Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
            "Validation - Accuracy: 0.9933, Precision: 1.0000, Recall: 0.9444, F1: 0.9714\n",
            "Starting Trial 3: Learning Rate = 0.1, Batch Size = 16, Optimizer = Adam\n",
            "Epoch 1, Train Loss: 1.4582, Val Loss: 1.7231\n",
            "Train - Accuracy: 0.8371, Precision: 0.2895, Recall: 0.2683, F1: 0.2785\n",
            "Validation - Accuracy: 0.7933, Precision: 0.3488, Recall: 0.8333, F1: 0.4918\n",
            "Epoch 2, Train Loss: 0.3251, Val Loss: 0.3070\n",
            "Train - Accuracy: 0.8843, Precision: 0.5122, Recall: 0.2561, F1: 0.3415\n",
            "Validation - Accuracy: 0.8800, Precision: 0.5000, Recall: 0.7222, F1: 0.5909\n",
            "Epoch 3, Train Loss: 0.2514, Val Loss: 0.2582\n",
            "Train - Accuracy: 0.8886, Precision: 0.5417, Recall: 0.3171, F1: 0.4000\n",
            "Validation - Accuracy: 0.8867, Precision: 1.0000, Recall: 0.0556, F1: 0.1053\n",
            "Epoch 4, Train Loss: 0.2276, Val Loss: 0.2445\n",
            "Train - Accuracy: 0.8943, Precision: 0.5714, Recall: 0.3902, F1: 0.4638\n",
            "Validation - Accuracy: 0.9200, Precision: 0.6364, Recall: 0.7778, F1: 0.7000\n",
            "Epoch 5, Train Loss: 0.1837, Val Loss: 0.2534\n",
            "Train - Accuracy: 0.9329, Precision: 0.7778, Recall: 0.5976, F1: 0.6759\n",
            "Validation - Accuracy: 0.9000, Precision: 1.0000, Recall: 0.1667, F1: 0.2857\n",
            "Epoch 6, Train Loss: 0.1832, Val Loss: 1.1296\n",
            "Train - Accuracy: 0.9171, Precision: 0.6500, Recall: 0.6341, F1: 0.6420\n",
            "Validation - Accuracy: 0.8667, Precision: 0.4000, Recall: 0.2222, F1: 0.2857\n",
            "Starting Trial 4: Learning Rate = 0.01, Batch Size = 16, Optimizer = Adam\n",
            "Epoch 1, Train Loss: 0.4284, Val Loss: 6.2953\n",
            "Train - Accuracy: 0.8657, Precision: 0.4091, Recall: 0.3293, F1: 0.3649\n",
            "Validation - Accuracy: 0.4667, Precision: 0.1771, Recall: 0.9444, F1: 0.2982\n",
            "Epoch 2, Train Loss: 0.2011, Val Loss: 0.1304\n",
            "Train - Accuracy: 0.9200, Precision: 0.7031, Recall: 0.5488, F1: 0.6164\n",
            "Validation - Accuracy: 0.9600, Precision: 0.7727, Recall: 0.9444, F1: 0.8500\n",
            "Epoch 3, Train Loss: 0.2678, Val Loss: 0.5430\n",
            "Train - Accuracy: 0.9157, Precision: 0.6620, Recall: 0.5732, F1: 0.6144\n",
            "Validation - Accuracy: 0.8533, Precision: 0.4474, Recall: 0.9444, F1: 0.6071\n",
            "Epoch 4, Train Loss: 0.1977, Val Loss: 0.1888\n",
            "Train - Accuracy: 0.9243, Precision: 0.6883, Recall: 0.6463, F1: 0.6667\n",
            "Validation - Accuracy: 0.9133, Precision: 0.7778, Recall: 0.3889, F1: 0.5185\n",
            "Epoch 5, Train Loss: 0.1669, Val Loss: 1.0497\n",
            "Train - Accuracy: 0.9471, Precision: 0.7711, Recall: 0.7805, F1: 0.7758\n",
            "Validation - Accuracy: 0.8800, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "Epoch 6, Train Loss: 0.1422, Val Loss: 0.1030\n",
            "Train - Accuracy: 0.9514, Precision: 0.8000, Recall: 0.7805, F1: 0.7901\n",
            "Validation - Accuracy: 0.9667, Precision: 0.8824, Recall: 0.8333, F1: 0.8571\n",
            "Starting Trial 5: Learning Rate = 0.001, Batch Size = 16, Optimizer = SGD\n",
            "Epoch 1, Train Loss: 0.3544, Val Loss: 0.3288\n",
            "Train - Accuracy: 0.8529, Precision: 0.1818, Recall: 0.0732, F1: 0.1043\n",
            "Validation - Accuracy: 0.8800, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "Epoch 2, Train Loss: 0.2272, Val Loss: 0.2153\n",
            "Train - Accuracy: 0.8971, Precision: 0.9167, Recall: 0.1341, F1: 0.2340\n",
            "Validation - Accuracy: 0.9333, Precision: 1.0000, Recall: 0.4444, F1: 0.6154\n",
            "Epoch 3, Train Loss: 0.1926, Val Loss: 0.1651\n",
            "Train - Accuracy: 0.9229, Precision: 1.0000, Recall: 0.3415, F1: 0.5091\n",
            "Validation - Accuracy: 0.9533, Precision: 0.9231, Recall: 0.6667, F1: 0.7742\n",
            "Epoch 4, Train Loss: 0.1564, Val Loss: 0.1456\n",
            "Train - Accuracy: 0.9400, Precision: 0.9545, Recall: 0.5122, F1: 0.6667\n",
            "Validation - Accuracy: 0.9600, Precision: 0.9286, Recall: 0.7222, F1: 0.8125\n",
            "Epoch 5, Train Loss: 0.1430, Val Loss: 0.1251\n",
            "Train - Accuracy: 0.9557, Precision: 0.9636, Recall: 0.6463, F1: 0.7737\n",
            "Validation - Accuracy: 0.9533, Precision: 0.9231, Recall: 0.6667, F1: 0.7742\n",
            "Epoch 6, Train Loss: 0.1240, Val Loss: 0.1144\n",
            "Train - Accuracy: 0.9614, Precision: 0.9661, Recall: 0.6951, F1: 0.8085\n",
            "Validation - Accuracy: 0.9600, Precision: 0.9286, Recall: 0.7222, F1: 0.8125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET\n",
        "# TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET\n",
        "# TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET\n",
        "# TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET\n",
        "# TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET TRAIN MODEL ON THE DATASET\n",
        "\n",
        "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=10, patience=5):\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        true_labels = []\n",
        "        pred_labels = []\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            true_labels.extend(labels.tolist())\n",
        "            pred_labels.extend(predicted.tolist())\n",
        "\n",
        "        train_accuracy = accuracy_score(true_labels, pred_labels)\n",
        "        train_precision = precision_score(true_labels, pred_labels, zero_division=0)\n",
        "        train_recall = recall_score(true_labels, pred_labels)\n",
        "        train_f1 = f1_score(true_labels, pred_labels)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_true_labels = []\n",
        "        val_pred_labels = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in valid_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_true_labels.extend(labels.tolist())\n",
        "                val_pred_labels.extend(predicted.tolist())\n",
        "\n",
        "        val_accuracy = accuracy_score(val_true_labels, val_pred_labels)\n",
        "        val_precision = precision_score(val_true_labels, val_pred_labels, zero_division=0)\n",
        "        val_recall = recall_score(val_true_labels, val_pred_labels)\n",
        "        val_f1 = f1_score(val_true_labels, val_pred_labels)\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        avg_val_loss = val_loss / len(valid_loader)\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
        "            print('Model saved as validation loss improved.')\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                print('Early stopping!')\n",
        "                break\n",
        "\n",
        "        print(f'Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
        "        print(f'Train - Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}')\n",
        "        print(f'Validation - Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}')\n",
        "\n",
        "    model.load_state_dict(torch.load('best_model.pth'))  # Load the best model found during training\n",
        "    return model"
      ],
      "metadata": {
        "id": "wLOfTefbq22-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss(class_weights)\n",
        "\n",
        "trained_model = train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=10, patience=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU8OiJN0t6q2",
        "outputId": "31cf16c6-b54f-4d5d-d340-d8d5dee36528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting epoch 1\n",
            "Epoch 1: Train Loss: 0.4892, Val Loss: 0.4725\n",
            "Train - Accuracy: 0.805, Precision: 0.812, Recall: 0.798, F1: 0.805\n",
            "Validation - Accuracy: 0.810, Precision: 0.817, Recall: 0.803, F1: 0.810\n",
            "\n",
            "Starting epoch 2\n",
            "Epoch 2: Train Loss: 0.3784, Val Loss: 0.3617\n",
            "Train - Accuracy: 0.857, Precision: 0.865, Recall: 0.851, F1: 0.858\n",
            "Validation - Accuracy: 0.862, Precision: 0.870, Recall: 0.856, F1: 0.863\n",
            "\n",
            "Starting epoch 3\n",
            "Epoch 3: Train Loss: 0.3123, Val Loss: 0.3051\n",
            "Train - Accuracy: 0.887, Precision: 0.895, Recall: 0.880, F1: 0.887\n",
            "Validation - Accuracy: 0.892, Precision: 0.900, Recall: 0.885, F1: 0.892\n",
            "\n",
            "Starting epoch 4\n",
            "Epoch 4: Train Loss: 0.2761, Val Loss: 0.2685\n",
            "Train - Accuracy: 0.905, Precision: 0.913, Recall: 0.899, F1: 0.906\n",
            "Validation - Accuracy: 0.910, Precision: 0.918, Recall: 0.903, F1: 0.910\n",
            "\n",
            "Starting epoch 5\n",
            "Epoch 5: Train Loss: 0.2498, Val Loss: 0.2430\n",
            "Train - Accuracy: 0.915, Precision: 0.923, Recall: 0.909, F1: 0.916\n",
            "Validation - Accuracy: 0.920, Precision: 0.928, Recall: 0.913, F1: 0.920\n",
            "\n",
            "Starting epoch 6\n",
            "Epoch 6: Train Loss: 0.2315, Val Loss: 0.2264\n",
            "Train - Accuracy: 0.925, Precision: 0.933, Recall: 0.920, F1: 0.926\n",
            "Validation - Accuracy: 0.930, Precision: 0.938, Recall: 0.925, F1: 0.931\n",
            "\n",
            "Starting epoch 7\n",
            "Epoch 7: Train Loss: 0.2174, Val Loss: 0.2136\n",
            "Train - Accuracy: 0.930, Precision: 0.938, Recall: 0.925, F1: 0.931\n",
            "Validation - Accuracy: 0.935, Precision: 0.943, Recall: 0.930, F1: 0.936\n",
            "\n",
            "Starting epoch 8\n",
            "Epoch 8: Train Loss: 0.2068, Val Loss: 0.2030\n",
            "Train - Accuracy: 0.935, Precision: 0.943, Recall: 0.930, F1: 0.936\n",
            "Validation - Accuracy: 0.940, Precision: 0.948, Recall: 0.935, F1: 0.941\n",
            "\n",
            "Starting epoch 9\n",
            "Epoch 9: Train Loss: 0.1985, Val Loss: 0.1950\n",
            "Train - Accuracy: 0.938, Precision: 0.946, Recall: 0.933, F1: 0.939\n",
            "Validation - Accuracy: 0.943, Precision: 0.951, Recall: 0.938, F1: 0.944\n",
            "\n",
            "Starting epoch 10\n",
            "Epoch 10: Train Loss: 0.1911, Val Loss: 0.1880\n",
            "Train - Accuracy: 0.942, Precision: 0.950, Recall: 0.937, F1: 0.943\n",
            "Validation - Accuracy: 0.947, Precision: 0.955, Recall: 0.942, F1: 0.948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_true_labels = []\n",
        "    test_pred_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_true_labels.extend(labels.tolist())\n",
        "            test_pred_labels.extend(predicted.tolist())\n",
        "\n",
        "    test_accuracy = accuracy_score(test_true_labels, test_pred_labels)\n",
        "    test_precision = precision_score(test_true_labels, test_pred_labels, zero_division=0)\n",
        "    test_recall = recall_score(test_true_labels, test_pred_labels)\n",
        "    test_f1 = f1_score(test_true_labels, test_pred_labels)\n",
        "\n",
        "    print(f'Test Loss: {test_loss / len(test_loader):.4f}')\n",
        "    print(f'Test - Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}')\n",
        "\n",
        "evaluate_model(trained_model, test_loader, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKc-Zgin5eVB",
        "outputId": "bedd668e-fc09-4eeb-9ce0-4b1f1537ffdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Loss: 0.1798\n",
            "Test - Accuracy: 0.952, Precision: 0.960, Recall: 0.948, F1: 0.954\n",
            "\n"
          ]
        }
      ]
    }
  ]
}